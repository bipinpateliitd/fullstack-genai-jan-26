{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Tutorial with LangChain\n",
    "\n",
    "This comprehensive tutorial covers various prompt engineering techniques using LangChain and ChatOpenAI.\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction & Setup\n",
    "2. Basic Prompt Techniques\n",
    "3. Template Variables & Formatting\n",
    "4. Prompting Techniques\n",
    "   - Zero-shot Prompting\n",
    "   - One-shot Prompting\n",
    "   - Few-shot Prompting\n",
    "   - Chain-of-Thought (CoT) Prompting\n",
    "   - Tree-of-Thought (ToT) Prompting\n",
    "   - Self-Consistency Prompting\n",
    "   - ReAct Prompting\n",
    "   - Role-based Prompting\n",
    "5. Output Parsers\n",
    "6. LCEL Chains\n",
    "7. Best Practices\n",
    "8. Real-world Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Setup\n",
    "\n",
    "**What is Prompt Engineering?**\n",
    "\n",
    "Prompt engineering is the practice of designing and optimizing prompts to get the best possible responses from Large Language Models (LLMs). It involves:\n",
    "- Crafting clear, specific instructions\n",
    "- Providing appropriate context and examples\n",
    "- Structuring prompts for desired output formats\n",
    "- Iterating and refining based on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Prompt Techniques\n",
    "\n",
    "### Simple Prompts with ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"What is the capital of France?\")\n",
    "])\n",
    "\n",
    "# Create chain and invoke\n",
    "chain = prompt | model | StrOutputParser()\n",
    "response = chain.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Types: System, Human, and AI Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different message types\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Python programming expert.\"),\n",
    "    HumanMessage(content=\"What is a list comprehension?\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Template Variables & Formatting\n",
    "\n",
    "Templates allow you to create reusable prompts with dynamic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template with variables\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"French\",\n",
    "    \"text\": \"I love programming\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompting Techniques\n",
    "\n",
    "### Zero-shot Prompting\n",
    "\n",
    "**Definition:** Direct instruction without any examples.\n",
    "\n",
    "**Importance:** Tests the model's inherent knowledge and capabilities.\n",
    "\n",
    "**Use Cases:** Simple tasks, general knowledge queries, when examples aren't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot prompting example\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a sentiment analysis expert.\"),\n",
    "    (\"human\", \"Classify the sentiment of this text as positive, negative, or neutral: {text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Test with different texts\n",
    "texts = [\n",
    "    \"I absolutely love this product! It's amazing!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The weather is cloudy today.\"\n",
    "]\n",
    "\n",
    "print(\"Zero-shot Sentiment Analysis:\\n\")\n",
    "for text in texts:\n",
    "    result = chain.invoke({\"text\": text})\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot Prompting\n",
    "\n",
    "**Definition:** Providing a single example to guide the model.\n",
    "\n",
    "**Importance:** Demonstrates desired format/style with minimal context.\n",
    "\n",
    "**Use Cases:** Format specification, style guidance, simple pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-shot prompting example\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an email classifier. Classify emails as 'spam' or 'not spam'.\"),\n",
    "    (\"human\", \"Email: Get rich quick! Click here now!\"),\n",
    "    (\"ai\", \"Classification: spam\"),\n",
    "    (\"human\", \"Email: {email}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "email = \"Meeting scheduled for tomorrow at 3 PM in conference room B.\"\n",
    "result = chain.invoke({\"email\": email})\n",
    "\n",
    "print(f\"Email: {email}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "**Definition:** Providing 2-5 examples to establish patterns.\n",
    "\n",
    "**Importance:** Significantly improves accuracy for specific tasks without fine-tuning.\n",
    "\n",
    "**Use Cases:** Classification, data extraction, consistent formatting, domain-specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompting example\n",
    "examples = [\n",
    "    {\"input\": \"The movie was fantastic! I loved every minute.\", \"output\": \"positive\"},\n",
    "    {\"input\": \"Terrible service, will never come back.\", \"output\": \"negative\"},\n",
    "    {\"input\": \"The product works as expected.\", \"output\": \"neutral\"},\n",
    "]\n",
    "\n",
    "# Create few-shot prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a sentiment classifier. Classify text as positive, negative, or neutral.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = final_prompt | model | StrOutputParser()\n",
    "\n",
    "# Test with new input\n",
    "new_text = \"The food was okay, nothing special.\"\n",
    "result = chain.invoke({\"input\": new_text})\n",
    "\n",
    "print(f\"Text: {new_text}\")\n",
    "print(f\"Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "**Definition:** Encouraging step-by-step reasoning.\n",
    "\n",
    "**Importance:** Dramatically improves performance on complex reasoning tasks.\n",
    "\n",
    "**Use Cases:** Math problems, logical reasoning, multi-step analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought prompting example\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a math tutor. Solve problems step by step.\"),\n",
    "    (\"human\", \"\"\"Solve this problem step by step:\n",
    "    \n",
    "A store has 45 apples. They sell 18 apples in the morning and 12 apples in the afternoon. \n",
    "Then they receive a delivery of 30 more apples. How many apples does the store have now?\n",
    "\n",
    "Let's think step by step:\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-Thought (ToT) Prompting\n",
    "\n",
    "**Definition:** Exploring multiple reasoning paths simultaneously.\n",
    "\n",
    "**Importance:** Enables exploration of different solution strategies.\n",
    "\n",
    "**Use Cases:** Complex problem-solving, creative tasks, strategic planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree-of-Thought prompting example\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a strategic problem solver.\"),\n",
    "    (\"human\", \"\"\"Problem: How can a small coffee shop increase revenue?\n",
    "\n",
    "Generate 3 different solution approaches:\n",
    "1. Approach A: Focus on...\n",
    "2. Approach B: Focus on...\n",
    "3. Approach C: Focus on...\n",
    "\n",
    "Then evaluate each approach and recommend the best one.\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency Prompting\n",
    "\n",
    "**Definition:** Generating multiple reasoning paths and selecting the most consistent answer.\n",
    "\n",
    "**Importance:** Improves reliability and reduces errors.\n",
    "\n",
    "**Use Cases:** Critical decisions, fact verification, complex reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-consistency prompting example\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a logical reasoning expert.\"),\n",
    "    (\"human\", \"\"\"Problem: If all roses are flowers, and some flowers fade quickly, \n",
    "can we conclude that some roses fade quickly?\n",
    "\n",
    "Solve this problem using 3 different reasoning approaches, then determine the most consistent answer.\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Prompting (Reasoning + Acting)\n",
    "\n",
    "**Definition:** Combining reasoning with tool/action execution.\n",
    "\n",
    "**Importance:** Enables agents to interact with external tools and APIs.\n",
    "\n",
    "**Use Cases:** Web search, database queries, API calls, multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct prompting example (simulated)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an AI assistant that uses the ReAct (Reasoning + Acting) framework.\n",
    "For each task, follow this pattern:\n",
    "1. Thought: Reason about what to do\n",
    "2. Action: Specify what action to take\n",
    "3. Observation: What you would observe from that action\n",
    "4. Repeat until you can provide a final answer\"\"\"),\n",
    "    (\"human\", \"Task: Find the current population of Tokyo and compare it to New York City.\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "result = chain.invoke({})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-based Prompting\n",
    "\n",
    "**Definition:** Assigning specific personas or expertise to the model.\n",
    "\n",
    "**Importance:** Tailors responses to specific domains and audiences.\n",
    "\n",
    "**Use Cases:** Expert advice, creative writing, technical documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role-based prompting example\n",
    "roles = [\n",
    "    \"You are a senior Python developer with 10 years of experience.\",\n",
    "    \"You are a creative writer specializing in science fiction.\",\n",
    "    \"You are a financial advisor helping clients with retirement planning.\"\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"What are the best practices for error handling?\",\n",
    "    \"Write a short story opening about time travel.\",\n",
    "    \"How should I diversify my investment portfolio?\"\n",
    "]\n",
    "\n",
    "for role, question in zip(roles, questions):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", role),\n",
    "        (\"human\", question)\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    result = chain.invoke({})\n",
    "    \n",
    "    print(f\"Role: {role}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {result[:200]}...\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output Parsers\n",
    "\n",
    "Output parsers help structure the model's responses into specific formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String output parser (already used above)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Tell me a fun fact about {topic}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "result = chain.invoke({\"topic\": \"space\"})\n",
    "\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Person's name\")\n",
    "    age: int = Field(description=\"Person's age\")\n",
    "    occupation: str = Field(description=\"Person's occupation\")\n",
    "    hobbies: List[str] = Field(description=\"List of hobbies\")\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract person information from the text.\\n{format_instructions}\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "text = \"\"\"John Smith is a 35-year-old software engineer. \n",
    "He enjoys hiking, photography, and playing guitar in his free time.\"\"\"\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"text\": text,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Name: {result.name}\")\n",
    "print(f\"Age: {result.age}\")\n",
    "print(f\"Occupation: {result.occupation}\")\n",
    "print(f\"Hobbies: {result.hobbies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON output parser\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Extract product information and return as JSON with these fields:\n",
    "- product_name\n",
    "- price\n",
    "- category\n",
    "- in_stock (boolean)\"\"\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | json_parser\n",
    "\n",
    "text = \"The iPhone 15 Pro costs $999 and is available in the Electronics category. Currently in stock.\"\n",
    "result = chain.invoke({\"text\": text})\n",
    "\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LCEL Chains\n",
    "\n",
    "LangChain Expression Language (LCEL) allows you to chain components using the pipe operator `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step chain example\n",
    "# Step 1: Generate a topic\n",
    "topic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a creative topic generator.\"),\n",
    "    (\"human\", \"Generate a random interesting topic about {subject}\")\n",
    "])\n",
    "\n",
    "# Step 2: Write about the topic\n",
    "writing_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a skilled writer.\"),\n",
    "    (\"human\", \"Write a brief paragraph about: {topic}\")\n",
    "])\n",
    "\n",
    "# Create chains\n",
    "topic_chain = topic_prompt | model | StrOutputParser()\n",
    "writing_chain = writing_prompt | model | StrOutputParser()\n",
    "\n",
    "# Execute\n",
    "subject = \"artificial intelligence\"\n",
    "topic = topic_chain.invoke({\"subject\": subject})\n",
    "paragraph = writing_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(f\"Generated Topic: {topic}\\n\")\n",
    "print(f\"Paragraph: {paragraph}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "### Clear and Specific Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad prompt (vague)\n",
    "bad_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Tell me about dogs\")\n",
    "])\n",
    "\n",
    "# Good prompt (specific)\n",
    "good_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"\"\"Provide 3 key characteristics of Golden Retrievers as family pets, \n",
    "focusing on temperament, exercise needs, and grooming requirements. \n",
    "Keep each point to 2 sentences.\"\"\")\n",
    "])\n",
    "\n",
    "print(\"Bad Prompt Result:\")\n",
    "result1 = (bad_prompt | model | StrOutputParser()).invoke({})\n",
    "print(result1[:200] + \"...\\n\")\n",
    "\n",
    "print(\"\\nGood Prompt Result:\")\n",
    "result2 = (good_prompt | model | StrOutputParser()).invoke({})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Write a creative opening line for a mystery novel.\")\n",
    "])\n",
    "\n",
    "temperatures = [0.0, 0.7, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    model_temp = ChatOpenAI(model=\"gpt-4o-mini\", temperature=temp)\n",
    "    chain = prompt | model_temp | StrOutputParser()\n",
    "    result = chain.invoke({})\n",
    "    print(f\"Temperature {temp}: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-world Examples\n",
    "\n",
    "### Content Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization example\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert at summarizing text concisely.\"),\n",
    "    (\"human\", \"Summarize the following text in 2-3 sentences:\\n\\n{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "long_text = \"\"\"Artificial Intelligence (AI) has transformed numerous industries over the past decade. \n",
    "From healthcare to finance, AI systems are being deployed to automate tasks, provide insights, \n",
    "and improve decision-making. Machine learning, a subset of AI, enables computers to learn from \n",
    "data without explicit programming. Deep learning, which uses neural networks with multiple layers, \n",
    "has achieved remarkable results in image recognition, natural language processing, and game playing. \n",
    "However, AI also raises important ethical questions about privacy, bias, and job displacement.\"\"\"\n",
    "\n",
    "summary = chain.invoke({\"text\": long_text})\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction with structured output\n",
    "class Event(BaseModel):\n",
    "    event_name: str = Field(description=\"Name of the event\")\n",
    "    date: str = Field(description=\"Date of the event\")\n",
    "    location: str = Field(description=\"Location of the event\")\n",
    "    attendees: int = Field(description=\"Number of attendees\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Event)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract event information from the text.\\n{format_instructions}\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "text = \"\"\"The Annual Tech Conference will be held on March 15, 2024, at the \n",
    "San Francisco Convention Center. We're expecting around 5000 attendees.\"\"\"\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"text\": text,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Event: {result.event_name}\")\n",
    "print(f\"Date: {result.date}\")\n",
    "print(f\"Location: {result.location}\")\n",
    "print(f\"Attendees: {result.attendees}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A with context\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant. Answer questions based only on the provided context. \n",
    "If the answer is not in the context, say \"I don't have enough information to answer that.\"\"\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "context = \"\"\"Python is a high-level, interpreted programming language created by Guido van Rossum \n",
    "and first released in 1991. It emphasizes code readability with significant whitespace. \n",
    "Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"When was Python first released?\",\n",
    "    \"What is Python's latest version?\"  # Not in context\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. **Basic Prompts**: ChatPromptTemplate and message types\n",
    "2. **Template Variables**: Dynamic prompt creation\n",
    "3. **Prompting Techniques**:\n",
    "   - Zero-shot: Direct instructions\n",
    "   - One-shot: Single example\n",
    "   - Few-shot: Multiple examples\n",
    "   - Chain-of-Thought: Step-by-step reasoning\n",
    "   - Tree-of-Thought: Multiple reasoning paths\n",
    "   - Self-Consistency: Multiple solutions\n",
    "   - ReAct: Reasoning + Acting\n",
    "   - Role-based: Expert personas\n",
    "4. **Output Parsers**: String, Pydantic, JSON\n",
    "5. **LCEL Chains**: Composing multiple steps\n",
    "6. **Best Practices**: Clear instructions, parameter tuning\n",
    "7. **Real-world Examples**: Summarization, extraction, Q&A\n",
    "\n",
    "### Key Takeaways:\n",
    "- Choose the right prompting technique based on your task complexity\n",
    "- Use few-shot prompting for better accuracy without fine-tuning\n",
    "- Chain-of-Thought improves reasoning on complex problems\n",
    "- Structure outputs with parsers for downstream processing\n",
    "- Always iterate and refine your prompts based on results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
