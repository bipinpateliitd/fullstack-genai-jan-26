{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Message Placeholders in LangChain Tutorial\n",
                "\n",
                "This comprehensive tutorial covers how to use placeholders in different types of messages when working with LLMs using LangChain and OpenAI.\n",
                "\n",
                "## What You'll Learn\n",
                "\n",
                "- **ChatPromptTemplate**: Building structured prompts with variables\n",
                "- **Message Types**: SystemMessage, HumanMessage, AIMessage with placeholders\n",
                "- **MessagesPlaceholder**: Dynamic conversation history insertion\n",
                "- **Practical Applications**: Real-world use cases and patterns\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- Python 3.8+\n",
                "- OpenAI API key\n",
                "- Basic understanding of LLMs and LangChain"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation\n",
                "\n",
                "First, let's install the required packages and set up our environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment if needed)\n",
                "# !pip install langchain-openai langchain-core python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Verify API key is loaded\n",
                "if not os.getenv(\"OPENAI_API_KEY\"):\n",
                "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
                "\n",
                "# Initialize the model\n",
                "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
                "\n",
                "print(\"âœ… Environment setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ChatPromptTemplate Basics\n",
                "\n",
                "### What are Placeholders?\n",
                "\n",
                "Placeholders are variables in your prompt templates that get replaced with actual values at runtime. They are denoted by curly braces `{variable_name}`.\n",
                "\n",
                "`ChatPromptTemplate` is the primary way to create structured prompts with placeholders in LangChain."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 1: Simple Single-Message Template"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple template with one variable\n",
                "chat_template = ChatPromptTemplate.from_template(\n",
                "    \"Tell me a joke about {topic}.\"\n",
                ")\n",
                "\n",
                "# Format and invoke\n",
                "messages = chat_template.format_messages(topic=\"programming\")\n",
                "\n",
                "print(\"Formatted Messages:\")\n",
                "for msg in messages:\n",
                "    print(f\"{type(msg).__name__}: {msg.content}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 2: Template with Multiple Variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Template with multiple variables\n",
                "chat_template = ChatPromptTemplate.from_template(\n",
                "    \"Translate the following {source_lang} text to {target_lang}: {text}\"\n",
                ")\n",
                "\n",
                "messages = chat_template.format_messages(\n",
                "    source_lang=\"English\",\n",
                "    target_lang=\"French\",\n",
                "    text=\"Hello, how are you?\"\n",
                ")\n",
                "\n",
                "print(\"Formatted Messages:\")\n",
                "for msg in messages:\n",
                "    print(f\"{type(msg).__name__}: {msg.content}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ChatPromptTemplate with Multiple Message Types\n",
                "\n",
                "The real power of `ChatPromptTemplate` comes from `from_messages()` which allows you to create structured conversations with different message types.\n",
                "\n",
                "### Message Type Tuples\n",
                "\n",
                "You can specify messages using tuples: `(\"role\", \"content with {placeholders}\")`\n",
                "\n",
                "Supported roles:\n",
                "- `\"system\"` - System messages (set AI behavior)\n",
                "- `\"human\"` or `\"user\"` - User messages\n",
                "- `\"ai\"` or `\"assistant\"` - AI assistant messages"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 3: System + Human Messages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a template with system and human messages\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a {role} who specializes in {specialty}. Your tone should be {tone}.\"),\n",
                "    (\"human\", \"{user_input}\")\n",
                "])\n",
                "\n",
                "# Format with values\n",
                "messages = chat_prompt.format_messages(\n",
                "    role=\"Python expert\",\n",
                "    specialty=\"data science\",\n",
                "    tone=\"friendly and educational\",\n",
                "    user_input=\"Explain what pandas is.\"\n",
                ")\n",
                "\n",
                "print(\"Formatted Messages:\")\n",
                "for msg in messages:\n",
                "    print(f\"\\n{type(msg).__name__}:\")\n",
                "    print(msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 4: Multi-Turn Conversation Template"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a multi-turn conversation template\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a {role}. Your expertise is in {domain}.\"),\n",
                "    (\"human\", \"I have a question about {topic}.\"),\n",
                "    (\"ai\", \"I'd be happy to help you with {topic}. What specifically would you like to know?\"),\n",
                "    (\"human\", \"{question}\")\n",
                "])\n",
                "\n",
                "messages = chat_prompt.format_messages(\n",
                "    role=\"senior software engineer\",\n",
                "    domain=\"cloud architecture\",\n",
                "    topic=\"AWS Lambda\",\n",
                "    question=\"What are the best practices for Lambda cold starts?\"\n",
                ")\n",
                "\n",
                "print(\"Formatted Conversation:\")\n",
                "for i, msg in enumerate(messages, 1):\n",
                "    print(f\"\\n{i}. {type(msg).__name__}:\")\n",
                "    print(msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 5: Complex System Message with Structured Content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Template with detailed, structured system message\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a {role}.\n",
                "\n",
                "Your responsibilities:\n",
                "- {responsibility_1}\n",
                "- {responsibility_2}\n",
                "- {responsibility_3}\n",
                "\n",
                "Communication style: {style}\n",
                "Target audience: {audience}\n",
                "\"\"\"),\n",
                "    (\"human\", \"{task}\")\n",
                "])\n",
                "\n",
                "messages = chat_prompt.format_messages(\n",
                "    role=\"technical documentation writer\",\n",
                "    responsibility_1=\"Explain complex concepts clearly\",\n",
                "    responsibility_2=\"Provide code examples\",\n",
                "    responsibility_3=\"Include best practices\",\n",
                "    style=\"concise and practical\",\n",
                "    audience=\"beginner developers\",\n",
                "    task=\"Explain what a REST API is.\"\n",
                ")\n",
                "\n",
                "print(\"Formatted Messages:\")\n",
                "for msg in messages:\n",
                "    print(f\"\\n{type(msg).__name__}:\")\n",
                "    print(msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 6: Few-Shot Learning Pattern"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Few-shot learning with examples\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a {task_type} assistant. Follow the pattern shown in the examples.\"),\n",
                "    (\"human\", \"{example_input_1}\"),\n",
                "    (\"ai\", \"{example_output_1}\"),\n",
                "    (\"human\", \"{example_input_2}\"),\n",
                "    (\"ai\", \"{example_output_2}\"),\n",
                "    (\"human\", \"{actual_input}\")\n",
                "])\n",
                "\n",
                "messages = chat_prompt.format_messages(\n",
                "    task_type=\"sentiment analysis\",\n",
                "    example_input_1=\"I love this product!\",\n",
                "    example_output_1=\"Positive\",\n",
                "    example_input_2=\"This is terrible.\",\n",
                "    example_output_2=\"Negative\",\n",
                "    actual_input=\"It's okay, nothing special.\"\n",
                ")\n",
                "\n",
                "print(\"Few-Shot Learning Template:\")\n",
                "for i, msg in enumerate(messages, 1):\n",
                "    print(f\"\\n{i}. {type(msg).__name__}: {msg.content}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. MessagesPlaceholder - Dynamic Conversation History\n",
                "\n",
                "`MessagesPlaceholder` is a special placeholder that allows you to insert a list of messages dynamically. This is crucial for maintaining conversation history."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 7: Basic MessagesPlaceholder Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a prompt with a placeholder for conversation history\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a helpful AI assistant. Remember the conversation context.\"),\n",
                "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
                "    (\"human\", \"{user_input}\")\n",
                "])\n",
                "\n",
                "# Simulate a conversation history\n",
                "conversation_history = [\n",
                "    HumanMessage(content=\"My name is Alice.\"),\n",
                "    AIMessage(content=\"Nice to meet you, Alice! How can I help you today?\"),\n",
                "    HumanMessage(content=\"I'm learning Python.\"),\n",
                "    AIMessage(content=\"That's great! Python is a wonderful language to learn. What aspect of Python are you focusing on?\")\n",
                "]\n",
                "\n",
                "# Format with history and new input\n",
                "messages = chat_prompt.format_messages(\n",
                "    chat_history=conversation_history,\n",
                "    user_input=\"What was my name again?\"\n",
                ")\n",
                "\n",
                "print(\"Full Conversation:\")\n",
                "for i, msg in enumerate(messages, 1):\n",
                "    print(f\"\\n{i}. {type(msg).__name__}:\")\n",
                "    print(msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Model Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 8: Interactive Conversation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a conversational loop\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a friendly chatbot. Keep track of the conversation.\"),\n",
                "    MessagesPlaceholder(variable_name=\"history\"),\n",
                "    (\"human\", \"{input}\")\n",
                "])\n",
                "\n",
                "# Initialize conversation history\n",
                "history = []\n",
                "\n",
                "# Simulate a multi-turn conversation\n",
                "user_inputs = [\n",
                "    \"Hi! I'm planning a trip to Japan.\",\n",
                "    \"What are the must-visit places?\",\n",
                "    \"How about food recommendations?\"\n",
                "]\n",
                "\n",
                "for user_input in user_inputs:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"User: {user_input}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    # Format messages with current history\n",
                "    messages = chat_prompt.format_messages(\n",
                "        history=history,\n",
                "        input=user_input\n",
                "    )\n",
                "    \n",
                "    # Get response\n",
                "    response = model.invoke(messages)\n",
                "    print(f\"Assistant: {response.content}\\n\")\n",
                "    \n",
                "    # Update history\n",
                "    history.append(HumanMessage(content=user_input))\n",
                "    history.append(AIMessage(content=response.content))\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"Total messages in history: {len(history)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 9: Optional MessagesPlaceholder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optional history - works with or without chat_history\n",
                "chat_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a helpful assistant.\"),\n",
                "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
                "    (\"human\", \"{question}\")\n",
                "])\n",
                "\n",
                "# First call - without history\n",
                "print(\"First call (no history):\")\n",
                "print(\"=\"*60)\n",
                "messages = chat_prompt.format_messages(\n",
                "    question=\"What is machine learning?\"\n",
                ")\n",
                "response1 = model.invoke(messages)\n",
                "print(response1.content)\n",
                "\n",
                "# Second call - with history\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Second call (with history):\")\n",
                "print(\"=\"*60)\n",
                "messages = chat_prompt.format_messages(\n",
                "    chat_history=[\n",
                "        HumanMessage(content=\"What is machine learning?\"),\n",
                "        AIMessage(content=response1.content)\n",
                "    ],\n",
                "    question=\"Can you give me an example?\"\n",
                ")\n",
                "response2 = model.invoke(messages)\n",
                "print(response2.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Practical Use Cases\n",
                "\n",
                "Let's explore real-world applications using ChatPromptTemplate."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case 1: Customer Support Chatbot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Customer support template\n",
                "support_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a customer support agent for {company_name}.\n",
                "Product: {product_name}\n",
                "Customer tier: {customer_tier}\n",
                "\n",
                "Guidelines:\n",
                "- Be professional and empathetic\n",
                "- Provide clear solutions\n",
                "- Escalate if needed\n",
                "\"\"\"),\n",
                "    MessagesPlaceholder(variable_name=\"conversation_history\"),\n",
                "    (\"human\", \"Customer issue: {issue}\")\n",
                "])\n",
                "\n",
                "# Simulate a support interaction\n",
                "messages = support_prompt.format_messages(\n",
                "    company_name=\"CloudHost Solutions\",\n",
                "    product_name=\"Premium Hosting Plan\",\n",
                "    customer_tier=\"Gold\",\n",
                "    conversation_history=[\n",
                "        HumanMessage(content=\"My website is down!\"),\n",
                "        AIMessage(content=\"I'm sorry to hear that. Let me check your server status immediately.\")\n",
                "    ],\n",
                "    issue=\"The website has been down for 30 minutes now.\"\n",
                ")\n",
                "\n",
                "print(\"Customer Support Conversation:\")\n",
                "for msg in messages:\n",
                "    print(f\"\\n{type(msg).__name__}:\")\n",
                "    print(msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Support Agent Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case 2: Code Review Assistant"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Code review template\n",
                "code_review_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are an expert code reviewer specializing in {language}.\n",
                "Focus areas: {focus_areas}\n",
                "Review style: {review_style}\n",
                "\n",
                "Provide constructive feedback on:\n",
                "1. Code quality\n",
                "2. Best practices\n",
                "3. Potential bugs\n",
                "4. Performance improvements\n",
                "\"\"\"),\n",
                "    (\"human\", \"\"\"Please review this code:\n",
                "\n",
                "```{language}\n",
                "{code}\n",
                "```\n",
                "\n",
                "Specific concerns: {concerns}\n",
                "\"\"\")\n",
                "])\n",
                "\n",
                "code_sample = \"\"\"def calculate_total(items):\n",
                "    total = 0\n",
                "    for item in items:\n",
                "        total = total + item['price'] * item['quantity']\n",
                "    return total\n",
                "\"\"\"\n",
                "\n",
                "messages = code_review_prompt.format_messages(\n",
                "    language=\"Python\",\n",
                "    focus_areas=\"performance, readability, error handling\",\n",
                "    review_style=\"detailed and educational\",\n",
                "    code=code_sample,\n",
                "    concerns=\"Is this function efficient? Should I add error handling?\"\n",
                ")\n",
                "\n",
                "print(\"Code Review Request:\")\n",
                "for msg in messages:\n",
                "    print(f\"\\n{type(msg).__name__}:\")\n",
                "    print(msg.content[:200] + \"...\" if len(msg.content) > 200 else msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Code Review:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case 3: Multi-Language Translation Service"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Translation template with context\n",
                "translation_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a professional translator.\n",
                "Source language: {source_lang}\n",
                "Target language: {target_lang}\n",
                "Context: {context}\n",
                "Tone: {tone}\n",
                "\n",
                "Provide accurate, culturally appropriate translations.\n",
                "\"\"\"),\n",
                "    (\"human\", \"Translate: {text}\")\n",
                "])\n",
                "\n",
                "# Example translations\n",
                "translations = [\n",
                "    {\n",
                "        \"source_lang\": \"English\",\n",
                "        \"target_lang\": \"Spanish\",\n",
                "        \"context\": \"business email\",\n",
                "        \"tone\": \"formal\",\n",
                "        \"text\": \"We appreciate your continued partnership.\"\n",
                "    },\n",
                "    {\n",
                "        \"source_lang\": \"English\",\n",
                "        \"target_lang\": \"Japanese\",\n",
                "        \"context\": \"casual conversation\",\n",
                "        \"tone\": \"friendly\",\n",
                "        \"text\": \"Let's grab coffee sometime!\"\n",
                "    }\n",
                "]\n",
                "\n",
                "for i, trans in enumerate(translations, 1):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Translation {i}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    messages = translation_prompt.format_messages(**trans)\n",
                "    response = model.invoke(messages)\n",
                "    \n",
                "    print(f\"Original ({trans['source_lang']}): {trans['text']}\")\n",
                "    print(f\"Translation ({trans['target_lang']}): {response.content}\")\n",
                "    print(f\"Context: {trans['context']}, Tone: {trans['tone']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case 4: RAG (Retrieval Augmented Generation) Pattern"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RAG template with context injection\n",
                "rag_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a helpful assistant that answers questions based on provided context.\n",
                "\n",
                "Instructions:\n",
                "- Use ONLY the information from the context below\n",
                "- If the answer is not in the context, say so\n",
                "- Cite specific parts of the context when answering\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\"\"\"),\n",
                "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
                "    (\"human\", \"{question}\")\n",
                "])\n",
                "\n",
                "# Simulate retrieved context\n",
                "context = \"\"\"LangChain is a framework for developing applications powered by language models.\n",
                "It provides several key features:\n",
                "1. Prompt templates for dynamic prompt construction\n",
                "2. Chains for combining multiple LLM calls\n",
                "3. Agents for decision-making and tool use\n",
                "4. Memory for maintaining conversation state\n",
                "5. Integration with various LLM providers like OpenAI, Anthropic, and more.\n",
                "\"\"\"\n",
                "\n",
                "# First question\n",
                "messages = rag_prompt.format_messages(\n",
                "    context=context,\n",
                "    question=\"What is LangChain?\"\n",
                ")\n",
                "\n",
                "print(\"Question 1: What is LangChain?\")\n",
                "print(\"=\"*60)\n",
                "response1 = model.invoke(messages)\n",
                "print(response1.content)\n",
                "\n",
                "# Follow-up question with history\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "print(\"Question 2: What are its key features?\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "messages = rag_prompt.format_messages(\n",
                "    context=context,\n",
                "    chat_history=[\n",
                "        HumanMessage(content=\"What is LangChain?\"),\n",
                "        AIMessage(content=response1.content)\n",
                "    ],\n",
                "    question=\"What are its key features?\"\n",
                ")\n",
                "\n",
                "response2 = model.invoke(messages)\n",
                "print(response2.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case 5: Content Generation with Style Guide"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Content generation with detailed style parameters\n",
                "content_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are a {content_type} writer.\n",
                "\n",
                "Style Guide:\n",
                "- Tone: {tone}\n",
                "- Target audience: {audience}\n",
                "- Length: {length}\n",
                "- Key themes: {themes}\n",
                "\n",
                "Writing rules:\n",
                "- {rule_1}\n",
                "- {rule_2}\n",
                "- {rule_3}\n",
                "\"\"\"),\n",
                "    (\"human\", \"Topic: {topic}\\n\\nAdditional instructions: {instructions}\")\n",
                "])\n",
                "\n",
                "messages = content_prompt.format_messages(\n",
                "    content_type=\"blog post\",\n",
                "    tone=\"professional yet approachable\",\n",
                "    audience=\"tech-savvy professionals\",\n",
                "    length=\"medium (300-400 words)\",\n",
                "    themes=\"innovation, practical applications, future trends\",\n",
                "    rule_1=\"Use concrete examples\",\n",
                "    rule_2=\"Avoid jargon unless explained\",\n",
                "    rule_3=\"Include actionable takeaways\",\n",
                "    topic=\"The Impact of AI on Software Development\",\n",
                "    instructions=\"Focus on how AI tools are changing daily workflows for developers.\"\n",
                ")\n",
                "\n",
                "print(\"Content Generation Request:\")\n",
                "print(\"=\"*60)\n",
                "for msg in messages:\n",
                "    print(f\"\\n{type(msg).__name__}:\")\n",
                "    print(msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Generated Content:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case 6: Interview Preparation Assistant"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interview preparation chatbot\n",
                "interview_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"\"\"You are an interview preparation coach.\n",
                "\n",
                "Interview Details:\n",
                "- Company: {company}\n",
                "- Position: {position}\n",
                "- Interview type: {interview_type}\n",
                "- Candidate background: {background}\n",
                "\n",
                "Your role:\n",
                "- Ask relevant interview questions\n",
                "- Provide constructive feedback\n",
                "- Suggest improvements\n",
                "\"\"\"),\n",
                "    MessagesPlaceholder(variable_name=\"interview_history\"),\n",
                "    (\"human\", \"{candidate_response}\")\n",
                "])\n",
                "\n",
                "# Simulate interview interaction\n",
                "messages = interview_prompt.format_messages(\n",
                "    company=\"TechCorp\",\n",
                "    position=\"Senior Python Developer\",\n",
                "    interview_type=\"technical\",\n",
                "    background=\"5 years experience in backend development\",\n",
                "    interview_history=[\n",
                "        AIMessage(content=\"Let's start with a technical question: Can you explain the difference between a list and a tuple in Python?\"),\n",
                "    ],\n",
                "    candidate_response=\"Lists are mutable and tuples are immutable. Lists use square brackets and tuples use parentheses.\"\n",
                ")\n",
                "\n",
                "print(\"Interview Simulation:\")\n",
                "print(\"=\"*60)\n",
                "for msg in messages:\n",
                "    print(f\"\\n{type(msg).__name__}:\")\n",
                "    print(msg.content)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Interviewer Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Advanced ChatPromptTemplate Patterns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pattern 1: Dynamic Template Building"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build templates dynamically based on conditions\n",
                "def create_assistant_prompt(include_examples=True, include_history=True):\n",
                "    messages = [\n",
                "        (\"system\", \"You are a {role} assistant.\")\n",
                "    ]\n",
                "    \n",
                "    if include_examples:\n",
                "        messages.extend([\n",
                "            (\"human\", \"{example_question}\"),\n",
                "            (\"ai\", \"{example_answer}\")\n",
                "        ])\n",
                "    \n",
                "    if include_history:\n",
                "        messages.append(MessagesPlaceholder(variable_name=\"history\", optional=True))\n",
                "    \n",
                "    messages.append((\"human\", \"{question}\"))\n",
                "    \n",
                "    return ChatPromptTemplate.from_messages(messages)\n",
                "\n",
                "# Create template with examples and history\n",
                "prompt = create_assistant_prompt(include_examples=True, include_history=True)\n",
                "\n",
                "messages = prompt.format_messages(\n",
                "    role=\"Python tutor\",\n",
                "    example_question=\"What is a decorator?\",\n",
                "    example_answer=\"A decorator is a function that modifies the behavior of another function.\",\n",
                "    question=\"Can you show me an example of a decorator?\"\n",
                ")\n",
                "\n",
                "print(\"Dynamic Template Example:\")\n",
                "print(\"=\"*60)\n",
                "for i, msg in enumerate(messages, 1):\n",
                "    print(f\"\\n{i}. {type(msg).__name__}: {msg.content}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
                "\n",
                "response = model.invoke(messages)\n",
                "print(\"Response:\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pattern 2: Template Chaining"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chain multiple prompts together\n",
                "# Step 1: Generate ideas\n",
                "idea_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a creative brainstorming assistant.\"),\n",
                "    (\"human\", \"Generate 3 {content_type} ideas about {topic}. List them briefly.\")\n",
                "])\n",
                "\n",
                "ideas_messages = idea_prompt.format_messages(\n",
                "    content_type=\"blog post\",\n",
                "    topic=\"sustainable living\"\n",
                ")\n",
                "\n",
                "print(\"Step 1: Generating Ideas\")\n",
                "print(\"=\"*60)\n",
                "ideas_response = model.invoke(ideas_messages)\n",
                "print(ideas_response.content)\n",
                "\n",
                "# Step 2: Expand on the best idea\n",
                "expand_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"You are a content writer.\"),\n",
                "    (\"human\", \"\"\"Here are some ideas:\n",
                "{ideas}\n",
                "\n",
                "Pick the most interesting one and write a detailed outline for it.\"\"\")\n",
                "])\n",
                "\n",
                "expand_messages = expand_prompt.format_messages(\n",
                "    ideas=ideas_response.content\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Step 2: Expanding Best Idea\")\n",
                "print(\"=\"*60)\n",
                "expand_response = model.invoke(expand_messages)\n",
                "print(expand_response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pattern 3: Reusable Template Factory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a reusable template factory class\n",
                "class PromptFactory:\n",
                "    \"\"\"Factory for creating common prompt templates.\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def create_qa_prompt(domain: str, style: str = \"concise\"):\n",
                "        \"\"\"Create a Q&A prompt for a specific domain.\"\"\"\n",
                "        return ChatPromptTemplate.from_messages([\n",
                "            (\"system\", f\"You are a {domain} expert. Answer questions in a {style} style.\"),\n",
                "            MessagesPlaceholder(variable_name=\"history\", optional=True),\n",
                "            (\"human\", \"{question}\")\n",
                "        ])\n",
                "    \n",
                "    @staticmethod\n",
                "    def create_task_prompt(task_type: str, requirements: list):\n",
                "        \"\"\"Create a task-oriented prompt.\"\"\"\n",
                "        req_text = \"\\n\".join([f\"- {req}\" for req in requirements])\n",
                "        return ChatPromptTemplate.from_messages([\n",
                "            (\"system\", f\"\"\"You are a {task_type} assistant.\n",
                "\n",
                "Requirements:\n",
                "{req_text}\n",
                "\"\"\"),\n",
                "            (\"human\", \"{task}\")\n",
                "        ])\n",
                "\n",
                "# Usage\n",
                "qa_prompt = PromptFactory.create_qa_prompt(\"machine learning\", \"detailed\")\n",
                "messages = qa_prompt.format_messages(\n",
                "    question=\"What is overfitting?\"\n",
                ")\n",
                "\n",
                "print(\"Reusable Template Factory Example:\")\n",
                "print(\"=\"*60)\n",
                "response = model.invoke(messages)\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Best Practices\n",
                "\n",
                "### Key Recommendations\n",
                "\n",
                "1. **Use `ChatPromptTemplate.from_messages()`**: This is the recommended way to create prompts\n",
                "2. **Clear Variable Names**: Use descriptive names like `user_question` instead of `q`\n",
                "3. **Consistent Formatting**: Maintain consistent placeholder style across templates\n",
                "4. **Template Reusability**: Create reusable templates for common patterns\n",
                "5. **Optional Placeholders**: Use `optional=True` in MessagesPlaceholder when appropriate\n",
                "6. **Structured System Messages**: Organize system messages with clear sections\n",
                "7. **Message Type Tuples**: Use simple tuples `(\"role\", \"content\")` for clarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best practices example\n",
                "import pandas as pd\n",
                "\n",
                "# Summary of common patterns\n",
                "patterns = {\n",
                "    'Pattern': [\n",
                "        'Simple Template',\n",
                "        'System + Human',\n",
                "        'Multi-Turn',\n",
                "        'Few-Shot Learning',\n",
                "        'With History',\n",
                "        'Optional History',\n",
                "        'RAG Pattern',\n",
                "        'Dynamic Building'\n",
                "    ],\n",
                "    'Use Case': [\n",
                "        'Single query/response',\n",
                "        'Setting AI behavior',\n",
                "        'Simulating conversation',\n",
                "        'Learning from examples',\n",
                "        'Stateful conversations',\n",
                "        'Flexible history handling',\n",
                "        'Context-based Q&A',\n",
                "        'Conditional templates'\n",
                "    ],\n",
                "    'Key Component': [\n",
                "        'from_template()',\n",
                "        '(\"system\", ...), (\"human\", ...)',\n",
                "        'Multiple message tuples',\n",
                "        'Example human/ai pairs',\n",
                "        'MessagesPlaceholder',\n",
                "        'optional=True',\n",
                "        'Context in system message',\n",
                "        'Dynamic message list'\n",
                "    ]\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(patterns)\n",
                "print(\"\\nChatPromptTemplate Patterns Summary:\")\n",
                "print(\"=\"*100)\n",
                "print(df.to_string(index=False))\n",
                "print(\"=\"*100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Conclusion\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **ChatPromptTemplate** is the primary tool for creating structured prompts\n",
                "2. **from_messages()** allows flexible multi-message templates\n",
                "3. **Message tuples** `(\"role\", \"content\")` are simple and powerful\n",
                "4. **MessagesPlaceholder** is essential for conversation history\n",
                "5. **Optional placeholders** provide flexibility\n",
                "6. **Template reusability** improves code maintainability\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Experiment with different message combinations\n",
                "- Build reusable template libraries for your use cases\n",
                "- Integrate with LangChain chains and agents\n",
                "- Explore prompt versioning and management\n",
                "- Implement conversation memory systems\n",
                "\n",
                "### Resources\n",
                "\n",
                "- [LangChain Prompts Documentation](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
                "- [ChatPromptTemplate API Reference](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n",
                "- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Practice Exercises\n",
                "\n",
                "Try these exercises to reinforce your understanding:\n",
                "\n",
                "1. **Exercise 1**: Create a job interview chatbot template with company, position, and interviewer style placeholders\n",
                "2. **Exercise 2**: Build a multi-turn conversation system with MessagesPlaceholder for a travel planning assistant\n",
                "3. **Exercise 3**: Create a few-shot learning template for sentiment analysis\n",
                "4. **Exercise 4**: Implement a RAG pattern template for a technical documentation Q&A system\n",
                "5. **Exercise 5**: Design a code generation template with language, framework, and requirements placeholders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise workspace - try your own templates here!\n",
                "\n",
                "# Example: Your custom template\n",
                "# my_prompt = ChatPromptTemplate.from_messages([\n",
                "#     (\"system\", \"...\"),\n",
                "#     (\"human\", \"...\")\n",
                "# ])\n",
                "\n",
                "# Test your template\n",
                "# messages = my_prompt.format_messages(...)\n",
                "# response = model.invoke(messages)\n",
                "# print(response.content)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}