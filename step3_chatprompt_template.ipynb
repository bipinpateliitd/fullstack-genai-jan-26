{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0679a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "load_dotenv(dotenv_path=\"/home/bipin/Documents/genai/g25-nov-hindi/langchain-learning/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acce817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = ChatOpenAI(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be54e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"what is AI\"\n",
    "output = model.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c3c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI, or Artificial Intelligence, refers to the development of computer systems and software that can perform tasks typically requiring human intelligence. These tasks include learning from data (machine learning), understanding natural language, recognizing images or speech, reasoning, problem-solving, and decision-making. AI can be categorized into narrow AI, which is designed for specific tasks (like voice assistants or recommendation systems), and general AI, which would have the ability to understand, learn, and apply intelligence across a wide range of activities similar to human cognition.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f6219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a29fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0696f58b",
   "metadata": {},
   "source": [
    " **ChatPromptTemplate**: Building structured prompts with variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d911aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(\"You are a helpful assistant.\")\n",
    "human_msg = HumanMessage(\"Hello, how are you?\")\n",
    "\n",
    "# Use with chat models\n",
    "messages = [system_msg, human_msg]\n",
    "response = model.invoke(messages)  # Returns AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b268e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing well, thank you. How can I assist you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea57dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "chain= model | StrOutputParser()\n",
    "response = chain.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af76001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc092ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f005f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e735b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ae851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86500fb",
   "metadata": {},
   "source": [
    "##  ChatPromptTemplate Basics\n",
    "\n",
    "### What are Placeholders?\n",
    "\n",
    "Placeholders are variables in your prompt templates that get replaced with actual values at runtime. They are denoted by curly braces `{variable_name}`.\n",
    "\n",
    "`ChatPromptTemplate` is the primary way to create structured prompts with placeholders in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e818e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d326a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0761e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =[(\"system\", \"You are a helpful assistant.\"),(\"human\", \"What is the capital of France?\")]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fdcbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48c813b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ec3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840d70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913409f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28e037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f63dbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =[(\"system\", \"You are a helpful assistant.\"),(\"human\", \"What is the capital of {country} ?\")]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8bf80f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country} ?'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97b7fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d5e1070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of the United States is Washington, D.C.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = chain2.invoke({\"country\":\"USA\"})\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8b5ab41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of the United States is Washington, D.C.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c7447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a3715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt using ChatPromptTemplate\n",
    "\n",
    "# Create chain and invoke\n",
    "chain = prompt | model \n",
    "response = chain.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ae6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d5682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following {source_lang} text to {target_lang}: {text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a6752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c2951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6da3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969b329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da508dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bcfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeeea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output =chain.invoke({\"source_lang\":\"English\",\"target_lang\":\"Hindi\",\"text\":\"Hello, how are you?\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"What is the capital of France?\")\n",
    "])\n",
    "\n",
    "# Create chain and invoke\n",
    "chain = prompt | model | StrOutputParser()\n",
    "response = chain.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0888b8",
   "metadata": {},
   "source": [
    "## 3. ChatPromptTemplate with Multiple Message Types\n",
    "\n",
    "The real power of `ChatPromptTemplate` comes from `from_messages()` which allows you to create structured conversations with different message types.\n",
    "\n",
    "### Message Type Tuples\n",
    "\n",
    "You can specify messages using tuples: `(\"role\", \"content with {placeholders}\")`\n",
    "\n",
    "Supported roles:\n",
    "- `\"system\"` - System messages (set AI behavior)\n",
    "- `\"human\"` or `\"user\"` - User messages\n",
    "- `\"ai\"` or `\"assistant\"` - AI assistant messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b994d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template with variables\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language} .\"),\n",
    "    (\"human\", \"{input_text}\")\n",
    "])\n",
    "prompt\n",
    "output = chain.invoke({\"input_language\": \"english\", \"input_text\": \"I love biryani\", \"output_language\" : \"telgu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6929a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input_language', 'input_text', 'output_language'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_text'], input_types={}, partial_variables={}, template='{input_text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a17eebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model |StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a83e780",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_language\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI love biryani\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_language\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtelugu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Note: \"telgu\" → \"telugu\"\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4880\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4865\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4866\u001b[39m \n\u001b[32m   4867\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4877\u001b[39m \n\u001b[32m   4878\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4879\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4880\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4881\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4886\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4887\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2058\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2054\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2056\u001b[39m         output = cast(\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2060\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2066\u001b[39m         )\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2068\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4737\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4735\u001b[39m                 output = chunk\n\u001b[32m   4736\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4737\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4739\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4740\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:117\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\n",
    "    \"input_language\": \"english\", \n",
    "    \"input_text\": \"I love biryani\", \n",
    "    \"output_language\": \"telugu\"  # Note: \"telgu\" → \"telugu\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea11068",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_language\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTelgu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_language\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI love biryani\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4880\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4865\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4866\u001b[39m \n\u001b[32m   4867\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4877\u001b[39m \n\u001b[32m   4878\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4879\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4880\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4881\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4886\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4887\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2058\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2054\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2056\u001b[39m         output = cast(\n\u001b[32m   2057\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2060\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2066\u001b[39m         )\n\u001b[32m   2067\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2068\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4737\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4735\u001b[39m                 output = chunk\n\u001b[32m   4736\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4737\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4739\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4740\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:435\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    434\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/genai/g25-nov-hindi/fullstack-genai-jan-26/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:117\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08d024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702a31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66604421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm glad you're interested in understanding how brain neurons work. Think of neurons as the tiny messengers of your brain—they're specialized cells that transmit information throughout your nervous system.\n",
      "\n",
      "Each neuron has three main parts:\n",
      "- **Dendrites**: these are branch-like structures that receive signals from other neurons.\n",
      "- **Cell body (soma)**: this processes the incoming signals.\n",
      "- **Axon**: a long, thin extension that sends signals to other neurons or muscles.\n",
      "\n",
      "Here's a gentle overview of how they work together:\n",
      "1. When a neuron receives enough signals from its dendrites, it generates an electrical impulse called an **action potential**.\n",
      "2. This impulse travels down the axon, often covered by a protective myelin sheath that speeds up transmission.\n",
      "3. When the impulse reaches the end of the axon, it triggers the release of chemical messengers called **neurotransmitters** into the tiny space called the synapse.\n",
      "4. These neurotransmitters then bind to receptors on neighboring neurons, passing the message along.\n",
      "\n",
      "This intricate network of neurons allows your brain to process thoughts, feelings, and actions seamlessly. It's quite a marvel how such small cells work together harmoniously to keep you functioning every day.\n",
      "\n",
      "If you'd like to learn more about specific aspects or have questions about neurological health, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role} who specializes in {specialty}. Your tone should be {tone}.\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"role\": \"Health expert\",\n",
    "    \"specialty\": \"Neurologist\",\n",
    "    \"tone\": \"soft\",\n",
    "    \"user_input\": \"How brain neurons work\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff3783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370baa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84cdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44c4ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content generation with detailed style parameters\n",
    "content_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a expert {content_type} writer.\n",
    "\n",
    "Style Guide:\n",
    "- Tone: {tone}\n",
    "- Target audience: {audience}\n",
    "- Length: {length}\n",
    "- Key themes: {themes}\n",
    "\n",
    "Writing rules:\n",
    "- {rule_1}\n",
    "- {rule_2}\n",
    "- {rule_3}\n",
    "\"\"\"),\n",
    "    (\"human\", \"Topic: {topic}\\n\\nAdditional instructions: {instructions}\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content_type=\"blog post\"\n",
    "tone=\"professional yet approachable\"\n",
    "audience=\"tech-savvy professionals\"\n",
    "length=\"medium (300-400 words)\"\n",
    "themes=\"innovation, practical applications, future trends\"\n",
    "rule_1=\"Use concrete examples\"\n",
    "rule_2=\"Avoid jargon unless explained\"\n",
    "rule_3=\"Include actionable takeaways\"\n",
    "topic=\"The Impact of AI on Software Development\"\n",
    "instructions=\"Focus on how AI tools are changing daily workflows for developers.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16ed9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= content_prompt |model |StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\n",
    "    \"content_type\":content_type,\n",
    "    \"tone\":tone,\n",
    "    \n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1023d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d8a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567af7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac5310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aff519",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert code reviewer specializing in {language}.\n",
    "Focus areas: {focus_areas}\n",
    "Review style: {review_style}\n",
    "\n",
    "Provide constructive feedback on:\n",
    "1. Code quality\n",
    "2. Best practices\n",
    "3. Potential bugs\n",
    "4. Performance improvements\n",
    "\"\"\"),\n",
    "    (\"human\", \"\"\"Please review this code:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Specific concerns: {concerns}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "code_sample = \"\"\"def calculate_total(items):\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total = total + item['price'] * item['quantity']\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "language=\"Python\"\n",
    "focus_areas=\"performance, readability, error handling\"\n",
    "review_style=\"detailed and educational\"\n",
    "code=code_sample,\n",
    "concerns=\"Is this function efficient? Should I add error handling?\"\n",
    "\n",
    "\n",
    "chain = code_review_prompt | model | StrOutputParser()\n",
    "response = chain.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dbdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e98b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a professional translator.\n",
    "Source language: {source_lang}\n",
    "Target language: {target_lang}\n",
    "Context: {context}\n",
    "Tone: {tone}\n",
    "\n",
    "Provide accurate, culturally appropriate translations.\n",
    "\"\"\"),\n",
    "    (\"human\", \"Translate: {text}\")\n",
    "])\n",
    "\n",
    "# Example translations\n",
    "translations = [\n",
    "    {\n",
    "        \"source_lang\": \"English\",\n",
    "        \"target_lang\": \"Spanish\",\n",
    "        \"context\": \"business email\",\n",
    "        \"tone\": \"formal\",\n",
    "        \"text\": \"We appreciate your continued partnership.\"\n",
    "    },\n",
    "    {\n",
    "        \"source_lang\": \"English\",\n",
    "        \"target_lang\": \"Japanese\",\n",
    "        \"context\": \"casual conversation\",\n",
    "        \"tone\": \"friendly\",\n",
    "        \"text\": \"Let's grab coffee sometime!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "chain = translation_prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab7135",
   "metadata": {},
   "source": [
    "## MessagesPlaceholder - Dynamic Conversation History\n",
    "\n",
    "`MessagesPlaceholder` is a special placeholder that allows you to insert a list of messages dynamically. This is crucial for maintaining conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5801050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with a placeholder for conversation history\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Remember the conversation context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Simulate a conversation history\n",
    "conversation_history = [\n",
    "    HumanMessage(content=\"My name is Ritesh.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Alice! How can I help you today?\"),\n",
    "    HumanMessage(content=\"I'm learning Python.\"),\n",
    "    AIMessage(content=\"That's great! Python is a wonderful language to learn. What aspect of Python are you focusing on?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | model | StrOutputParser()\n",
    "response= chain.invoke({\"chat_history\": conversation_history, \"user_input\": \"what is my name?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer support template\n",
    "support_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a customer support agent for {company_name}.\n",
    "Product: {product_name}\n",
    "Customer tier: {customer_tier}\n",
    "\n",
    "Guidelines:\n",
    "- Be professional and empathetic\n",
    "- Provide clear solutions\n",
    "- Escalate if needed\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"conversation_history\"),\n",
    "    (\"human\", \"Customer issue: {issue}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name=\"CloudHost Solutions\"\n",
    "product_name=\"Premium Hosting Plan\"\n",
    "customer_tier=\"Gold\",\n",
    "conversation_history=[\n",
    "    HumanMessage(content=\"My website is down!\"),\n",
    "    AIMessage(content=\"I'm sorry to hear that. Let me check your server status immediately.\")\n",
    "]\n",
    "issue=\"The website has been down for 30 minutes now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an interview preparation coach.\n",
    "\n",
    "Interview Details:\n",
    "- Company: {company}\n",
    "- Position: {position}\n",
    "- Interview type: {interview_type}\n",
    "- Candidate background: {background}\n",
    "\n",
    "Your role:\n",
    "- Ask relevant interview questions\n",
    "- Provide constructive feedback\n",
    "- Suggest improvements\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"interview_history\"),\n",
    "    (\"human\", \"{candidate_response}\")\n",
    "])\n",
    "\n",
    "# Simulate interview interaction\n",
    "\n",
    "company=\"TechCorp\",\n",
    "position=\"Senior Python Developer\",\n",
    "interview_type=\"technical\",\n",
    "background=\"5 years experience in backend development\",\n",
    "interview_history=[\n",
    "    AIMessage(content=\"Let's start with a technical question: Can you explain the difference between a list and a tuple in Python?\"),\n",
    "],\n",
    "candidate_response=\"Lists are mutable and tuples are immutable. Lists use square brackets and tuples use parentheses.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
